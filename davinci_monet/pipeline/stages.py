"""Pipeline stage definitions.

This module provides the Stage protocol and concrete stage implementations
for the analysis pipeline. Each stage is a composable unit of work that
transforms data through the analysis workflow.
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Any, Callable, Protocol, Sequence, TypeVar, runtime_checkable

import xarray as xr

from davinci_monet.core.exceptions import PipelineError
from davinci_monet.core.protocols import DataGeometry


class StageStatus(Enum):
    """Status of a pipeline stage."""

    PENDING = auto()
    RUNNING = auto()
    COMPLETED = auto()
    FAILED = auto()
    SKIPPED = auto()


@dataclass
class StageResult:
    """Result of a pipeline stage execution.

    Attributes
    ----------
    stage_name
        Name of the stage that produced this result.
    status
        Execution status.
    data
        Output data from the stage.
    metadata
        Additional metadata about the execution.
    error
        Error message if the stage failed.
    duration_seconds
        Execution time in seconds.
    """

    stage_name: str
    status: StageStatus
    data: Any = None
    metadata: dict[str, Any] = field(default_factory=dict)
    error: str | None = None
    duration_seconds: float = 0.0


@runtime_checkable
class Stage(Protocol):
    """Protocol for pipeline stages.

    A stage is a single unit of work in the analysis pipeline.
    Stages can be composed and chained together.
    """

    @property
    def name(self) -> str:
        """Stage name."""
        ...

    def execute(self, context: PipelineContext) -> StageResult:
        """Execute the stage.

        Parameters
        ----------
        context
            Pipeline context containing configuration and data.

        Returns
        -------
        StageResult
            Result of stage execution.
        """
        ...

    def validate(self, context: PipelineContext) -> bool:
        """Validate that the stage can run with the given context.

        Parameters
        ----------
        context
            Pipeline context to validate.

        Returns
        -------
        bool
            True if validation passes.
        """
        ...


@dataclass
class PipelineContext:
    """Context passed between pipeline stages.

    Contains configuration, data, and state that flows through the pipeline.

    Attributes
    ----------
    config
        Configuration dictionary from YAML or programmatic setup.
    models
        Dictionary of loaded model data.
    observations
        Dictionary of loaded observation data.
    paired
        Dictionary of paired model-observation data.
    results
        Results from completed stages.
    metadata
        Pipeline metadata (start time, etc.).
    """

    config: dict[str, Any] = field(default_factory=dict)
    models: dict[str, Any] = field(default_factory=dict)
    observations: dict[str, Any] = field(default_factory=dict)
    paired: dict[str, Any] = field(default_factory=dict)
    results: dict[str, StageResult] = field(default_factory=dict)
    metadata: dict[str, Any] = field(default_factory=dict)

    def get_model(self, label: str) -> Any:
        """Get a model by label."""
        if label not in self.models:
            raise KeyError(f"Model '{label}' not found in context")
        return self.models[label]

    def get_observation(self, label: str) -> Any:
        """Get an observation by label."""
        if label not in self.observations:
            raise KeyError(f"Observation '{label}' not found in context")
        return self.observations[label]

    def get_paired(self, key: str) -> Any:
        """Get paired data by key."""
        if key not in self.paired:
            raise KeyError(f"Paired data '{key}' not found in context")
        return self.paired[key]


class BaseStage(ABC):
    """Abstract base class for pipeline stages.

    Provides common functionality for stage implementations.
    """

    def __init__(self, name: str | None = None) -> None:
        """Initialize stage.

        Parameters
        ----------
        name
            Optional custom name. If None, uses class name.
        """
        self._name = name or self.__class__.__name__

    @property
    def name(self) -> str:
        """Stage name."""
        return self._name

    def validate(self, context: PipelineContext) -> bool:
        """Default validation - always passes.

        Override in subclasses for specific validation.
        """
        return True

    @abstractmethod
    def execute(self, context: PipelineContext) -> StageResult:
        """Execute the stage."""
        ...

    def _create_result(
        self,
        status: StageStatus,
        data: Any = None,
        error: str | None = None,
        duration: float = 0.0,
        **metadata: Any,
    ) -> StageResult:
        """Create a stage result."""
        return StageResult(
            stage_name=self.name,
            status=status,
            data=data,
            error=error,
            duration_seconds=duration,
            metadata=metadata,
        )


class LoadModelsStage(BaseStage):
    """Stage for loading model data.

    Reads model configuration and loads model files into the context.
    """

    def __init__(self) -> None:
        super().__init__("load_models")

    def validate(self, context: PipelineContext) -> bool:
        """Validate that model config exists."""
        return "model" in context.config or "models" in context.config

    def execute(self, context: PipelineContext) -> StageResult:
        """Load model data from configuration."""
        import time

        from davinci_monet.models import open_model

        start = time.time()
        model_config = context.config.get("model") or context.config.get("models", {})

        loaded_count = 0
        for label, config in model_config.items():
            try:
                files = config.get("files", config.get("filename"))
                mod_type = config.get("mod_type", "generic")
                variables = config.get("variables")

                if isinstance(variables, dict):
                    var_list = list(variables.keys())
                else:
                    var_list = variables

                model_data = open_model(
                    files=files,
                    mod_type=mod_type,
                    variables=var_list,
                    label=label,
                )

                # Apply unit scaling and units if configured
                if isinstance(variables, dict):
                    for var_name, var_config in variables.items():
                        if isinstance(var_config, dict):
                            if "unit_scale" in var_config:
                                scale = var_config["unit_scale"]
                                method = var_config.get("unit_scale_method", "*")
                                if var_name in model_data.data.data_vars:
                                    model_data.apply_unit_scale(var_name, scale, method)
                            if "units" in var_config and var_name in model_data.data.data_vars:
                                model_data.data[var_name].attrs["units"] = var_config["units"]

                context.models[label] = model_data
                loaded_count += 1
            except Exception as e:
                return self._create_result(
                    StageStatus.FAILED,
                    error=f"Failed to load model '{label}': {e}",
                    duration=time.time() - start,
                )

        return self._create_result(
            StageStatus.COMPLETED,
            data={"loaded_models": list(context.models.keys())},
            duration=time.time() - start,
            count=loaded_count,
        )


class LoadObservationsStage(BaseStage):
    """Stage for loading observation data.

    Reads observation configuration and loads observation files into the context.
    """

    def __init__(self) -> None:
        super().__init__("load_observations")

    def validate(self, context: PipelineContext) -> bool:
        """Validate that observation config exists."""
        return "obs" in context.config or "observations" in context.config

    def execute(self, context: PipelineContext) -> StageResult:
        """Load observation data from configuration."""
        import time
        from glob import glob
        from pathlib import Path

        import xarray as xr

        from davinci_monet.observations import create_observation_data

        start = time.time()
        obs_config = context.config.get("obs") or context.config.get("observations", {})

        # Use current working directory for relative paths
        base_path = Path.cwd()

        loaded_count = 0
        for label, config in obs_config.items():
            try:
                obs_type = config.get("obs_type", "pt_sfc")
                filename = config.get("filename")
                variables = config.get("variables", {})

                # Load data from file
                data = None
                if filename:
                    file_path = Path(filename)
                    # Handle relative paths
                    if not file_path.is_absolute():
                        file_path = base_path / file_path

                    # Expand user home directory
                    file_path = file_path.expanduser()

                    # Handle glob patterns
                    if "*" in str(file_path) or "?" in str(file_path):
                        files = sorted(glob(str(file_path)))
                        if files:
                            data = xr.open_mfdataset(files, combine="by_coords")
                    elif file_path.exists():
                        data = xr.open_dataset(str(file_path))

                obs_data = create_observation_data(
                    label=label,
                    obs_type=obs_type,
                    data=data,
                    filename=filename,
                    variables=variables,
                )

                # Apply unit scaling and units if configured
                if isinstance(variables, dict):
                    for var_name, var_config in variables.items():
                        if isinstance(var_config, dict):
                            if "unit_scale" in var_config:
                                scale = var_config["unit_scale"]
                                method = var_config.get("unit_scale_method", "*")
                                if var_name in obs_data.data.data_vars:
                                    obs_data.apply_unit_scale(var_name, scale, method)
                            if "units" in var_config and var_name in obs_data.data.data_vars:
                                obs_data.data[var_name].attrs["units"] = var_config["units"]

                context.observations[label] = obs_data
                loaded_count += 1
            except Exception as e:
                return self._create_result(
                    StageStatus.FAILED,
                    error=f"Failed to load observation '{label}': {e}",
                    duration=time.time() - start,
                )

        return self._create_result(
            StageStatus.COMPLETED,
            data={"loaded_observations": list(context.observations.keys())},
            duration=time.time() - start,
            count=loaded_count,
        )


class PairingStage(BaseStage):
    """Stage for pairing model and observation data.

    Uses the pairing engine to match model output with observations.
    """

    def __init__(self) -> None:
        super().__init__("pairing")

    def validate(self, context: PipelineContext) -> bool:
        """Validate that models and observations are loaded."""
        return bool(context.models) and bool(context.observations)

    def execute(self, context: PipelineContext) -> StageResult:
        """Pair model and observation data."""
        import time

        from davinci_monet.pairing import PairingEngine, PairingConfig

        start = time.time()
        paired_count = 0

        # Get pairing configuration
        pairing_config_dict = context.config.get("pairing", {})

        engine = PairingEngine()

        for model_label, model_data in context.models.items():
            for obs_label, obs_data in context.observations.items():
                try:
                    pair_key = f"{model_label}_{obs_label}"

                    # Get model-obs variable mapping
                    model_config = context.config.get("model", {}).get(model_label, {})
                    mapping = model_config.get("mapping", {})
                    if mapping and obs_label not in mapping:
                        continue

                    # Extract variable mappings: {obs_var: model_var}
                    var_mapping = mapping.get(obs_label, {})
                    if not var_mapping:
                        continue

                    obs_vars = list(var_mapping.keys())
                    model_vars = list(var_mapping.values())

                    # Get model and obs datasets
                    model_ds = model_data.data if hasattr(model_data, "data") else model_data
                    obs_ds = obs_data.data if hasattr(obs_data, "data") else obs_data

                    if model_ds is None or obs_ds is None:
                        continue

                    # Build pairing config
                    radius = model_config.get("radius_of_influence", 12000.0)
                    pairing_cfg = PairingConfig(
                        radius_of_influence=radius,
                        time_tolerance=pairing_config_dict.get("time_tolerance", "1h"),
                    )

                    # Pair data
                    paired_ds = engine.pair(
                        model_ds,
                        obs_ds,
                        obs_vars=obs_vars,
                        model_vars=model_vars,
                        config=pairing_cfg,
                    )

                    context.paired[pair_key] = paired_ds
                    paired_count += 1

                except Exception as e:
                    # Log but continue with other pairs
                    context.metadata.setdefault("pairing_errors", []).append(
                        f"{pair_key}: {e}"
                    )

        return self._create_result(
            StageStatus.COMPLETED,
            data={"paired_keys": list(context.paired.keys())},
            duration=time.time() - start,
            count=paired_count,
        )


class StatisticsStage(BaseStage):
    """Stage for calculating statistics on paired data."""

    def __init__(self) -> None:
        super().__init__("statistics")

    def validate(self, context: PipelineContext) -> bool:
        """Validate that paired data exists."""
        return bool(context.paired)

    def execute(self, context: PipelineContext) -> StageResult:
        """Calculate statistics on paired data."""
        import time

        start = time.time()
        stats_results: dict[str, Any] = {}

        stats_config = context.config.get("stats", {})

        for pair_key, paired_obj in context.paired.items():
            try:
                # Handle PairedData objects
                paired_data = paired_obj.data if hasattr(paired_obj, "data") else paired_obj
                # Calculate basic statistics
                pair_stats = self._calculate_stats(paired_data, stats_config)
                stats_results[pair_key] = pair_stats
            except Exception as e:
                context.metadata.setdefault("stats_errors", []).append(
                    f"{pair_key}: {e}"
                )

        return self._create_result(
            StageStatus.COMPLETED,
            data=stats_results,
            duration=time.time() - start,
        )

    def _calculate_stats(
        self, paired_data: xr.Dataset, config: dict[str, Any]
    ) -> dict[str, Any]:
        """Calculate statistics for a paired dataset."""
        import numpy as np

        stats: dict[str, Any] = {}

        # Find model and obs variable pairs (prefix format: model_*, obs_*)
        model_vars = [v for v in paired_data.data_vars if v.startswith("model_")]

        for model_var in model_vars:
            base_name = model_var.replace("model_", "", 1)
            obs_var = f"obs_{base_name}"

            if obs_var not in paired_data:
                continue

            model_vals = paired_data[model_var].values.flatten()
            obs_vals = paired_data[obs_var].values.flatten()

            # Remove NaNs
            mask = ~(np.isnan(model_vals) | np.isnan(obs_vals))
            model_vals = model_vals[mask]
            obs_vals = obs_vals[mask]

            if len(model_vals) == 0:
                continue

            # Calculate metrics
            diff = model_vals - obs_vals
            stats[base_name] = {
                "n": len(model_vals),
                "mean_bias": float(np.mean(diff)),
                "rmse": float(np.sqrt(np.mean(diff**2))),
                "correlation": float(np.corrcoef(model_vals, obs_vals)[0, 1])
                if len(model_vals) > 1
                else np.nan,
                "model_mean": float(np.mean(model_vals)),
                "obs_mean": float(np.mean(obs_vals)),
            }

        return stats


class PlottingStage(BaseStage):
    """Stage for generating plots from paired data."""

    def __init__(self) -> None:
        super().__init__("plotting")

    def validate(self, context: PipelineContext) -> bool:
        """Validate that paired data exists."""
        return bool(context.paired)

    def execute(self, context: PipelineContext) -> StageResult:
        """Generate plots from paired data."""
        import time
        from pathlib import Path

        import matplotlib.pyplot as plt

        from davinci_monet.plots import get_plotter

        start = time.time()
        plots_generated: list[str] = []

        plot_config = context.config.get("plots", {})

        # Plotting is optional - if no config, skip
        if not plot_config:
            return self._create_result(
                StageStatus.SKIPPED,
                data={"message": "No plot configuration found"},
                duration=time.time() - start,
            )

        # Get output directory
        analysis_config = context.config.get("analysis", {})
        output_dir = Path(analysis_config.get("output_dir", "."))
        output_dir.mkdir(parents=True, exist_ok=True)

        # Get pairs config for variable mapping
        pairs_config = context.config.get("pairs", {})
        model_config = context.config.get("model", {})

        for plot_name, plot_spec in plot_config.items():
            try:
                plot_type = plot_spec.get("type", "scatter")
                plot_pairs = plot_spec.get("pairs", [])
                title = plot_spec.get("title", plot_name)

                for pair_name in plot_pairs:
                    # Get pair configuration
                    pair_spec = pairs_config.get(pair_name, {})
                    model_label = pair_spec.get("model", "")
                    obs_label = pair_spec.get("obs", "")
                    var_spec = pair_spec.get("variable", {})
                    obs_var = var_spec.get("obs_var", "")

                    # Find paired data
                    pair_key = f"{model_label}_{obs_label}"
                    if pair_key not in context.paired:
                        continue

                    paired_obj = context.paired[pair_key]
                    paired_data = paired_obj.data if hasattr(paired_obj, "data") else paired_obj

                    # Variable names in paired dataset use obs_var with prefixes
                    obs_var_name = f"obs_{obs_var}"
                    model_var_name = f"model_{obs_var}"

                    if obs_var_name not in paired_data or model_var_name not in paired_data:
                        continue

                    # Get plotter config from model variable settings
                    model_var = var_spec.get("model_var", "")
                    var_config = model_config.get(model_label, {}).get("variables", {}).get(model_var, {})
                    vmin = var_config.get("vmin_plot")
                    vmax = var_config.get("vmax_plot")
                    vdiff = var_config.get("vdiff_plot")

                    # Build plotter config
                    plotter_config = {"title": title}
                    if plot_type == "spatial_bias":
                        plotter_config["vmin"] = -vdiff if vdiff else None
                        plotter_config["vmax"] = vdiff if vdiff else None
                    elif plot_type == "timeseries":
                        # Timeseries uses smart auto-scaling from data range
                        # Don't pass vmin/vmax unless explicitly set in plot_spec
                        if "vmin" in plot_spec:
                            plotter_config["vmin"] = plot_spec["vmin"]
                        if "vmax" in plot_spec:
                            plotter_config["vmax"] = plot_spec["vmax"]
                    else:
                        plotter_config["vmin"] = vmin
                        plotter_config["vmax"] = vmax

                    # Extract additional plot options from plot_spec
                    plot_options: dict[str, Any] = {}
                    for opt_key in ["show_site_labels", "show_individual_sites",
                                    "show_uncertainty", "uncertainty_type",
                                    "resample", "aggregate_dim", "label_sites",
                                    "site_label_var", "city_labels"]:
                        if opt_key in plot_spec:
                            plot_options[opt_key] = plot_spec[opt_key]

                    # Add city_labels from analysis config for spatial plots
                    if plot_type.startswith("spatial") and "city_labels" not in plot_options:
                        city_labels = analysis_config.get("city_labels")
                        if city_labels:
                            plot_options["city_labels"] = city_labels

                    # Get plotter and generate plot
                    plotter = get_plotter(plot_type, config=plotter_config)
                    fig = plotter.plot(paired_data, obs_var_name, model_var_name, **plot_options)

                    # Save plot
                    output_path = output_dir / f"{plot_name}.png"
                    plotter.save(fig, output_path, dpi=300)
                    plots_generated.append(str(output_path))

                    # Also save PDF
                    pdf_path = output_dir / f"{plot_name}.pdf"
                    plotter.save(fig, pdf_path)
                    plots_generated.append(str(pdf_path))

                    plt.close(fig)

            except Exception as e:
                context.metadata.setdefault("plot_errors", []).append(
                    f"{plot_name}: {e}"
                )

        return self._create_result(
            StageStatus.COMPLETED,
            data={"plots_generated": plots_generated},
            duration=time.time() - start,
        )


class SaveResultsStage(BaseStage):
    """Stage for saving analysis results."""

    def __init__(self) -> None:
        super().__init__("save_results")

    def execute(self, context: PipelineContext) -> StageResult:
        """Save analysis results to files."""
        import time
        from pathlib import Path

        import pandas as pd

        start = time.time()
        saved_files: list[str] = []

        # Get output directory from analysis config
        analysis_config = context.config.get("analysis", {})
        output_dir = Path(analysis_config.get("output_dir", "."))
        output_dir.mkdir(parents=True, exist_ok=True)

        # Save statistics summary from statistics stage
        stats_result = context.results.get("statistics")
        if stats_result and stats_result.data:
            rows = []
            for pair_key, pair_stats in stats_result.data.items():
                for var_name, var_stats in pair_stats.items():
                    row = {"Variable": var_name}
                    row["N"] = var_stats.get("n", 0)
                    row["Mean_Obs"] = var_stats.get("obs_mean", float("nan"))
                    row["Mean_Model"] = var_stats.get("model_mean", float("nan"))
                    row["MB"] = var_stats.get("mean_bias", float("nan"))
                    row["RMSE"] = var_stats.get("rmse", float("nan"))
                    row["R"] = var_stats.get("correlation", float("nan"))
                    # Calculate NMB and NME
                    obs_mean = var_stats.get("obs_mean", 0)
                    if obs_mean != 0:
                        row["NMB_%"] = (var_stats.get("mean_bias", 0) / obs_mean) * 100
                        row["NME_%"] = (var_stats.get("rmse", 0) / abs(obs_mean)) * 100
                    else:
                        row["NMB_%"] = float("nan")
                        row["NME_%"] = float("nan")
                    row["IOA"] = var_stats.get("ioa", float("nan"))
                    rows.append(row)

            if rows:
                df = pd.DataFrame(rows)
                df = df.set_index("Variable")
                stats_file = output_dir / "statistics_summary.csv"
                df.to_csv(stats_file)
                saved_files.append(str(stats_file))

        return self._create_result(
            StageStatus.COMPLETED,
            data={"saved_files": saved_files},
            duration=time.time() - start,
        )


# Convenience function to create a standard analysis pipeline
def create_standard_pipeline() -> list[BaseStage]:
    """Create a standard analysis pipeline with all stages.

    Returns
    -------
    list[BaseStage]
        List of stages for a complete analysis.
    """
    return [
        LoadModelsStage(),
        LoadObservationsStage(),
        PairingStage(),
        StatisticsStage(),
        PlottingStage(),
        SaveResultsStage(),
    ]
